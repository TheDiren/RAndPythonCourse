{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PANGAEA_Banner.png](https://gitlab.awi.de/kriemann/nfdi4earth_academy_data/raw/main/logo/PANGAEA_Banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to download data from PANGAEA and manipulate it \n",
    "\n",
    "based on [PANGAEA Community Workshop script](Python/PANGAEApy_practical/pangaeapy_practical_solutions.ipynb)  \n",
    "Last updated: 2025-08-20  \n",
    "\n",
    "This notebook will guide you how to search and retrieve diverse earth- and environmental data and its metadata from the [PANGAEA data repository](https://www.pangaea.de) using Python. It uses the [PANGAEApy package](https://pypi.org/project/pangaeapy/), version 1.0.22 to facilitate the data download. \n",
    "\n",
    "Check out our [Wiki](https://wiki.pangaea.de/wiki/PANGAEA_search) for further details on searching data in PANGAEA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### PANGAEApy\n",
    "## if you need to install PANGAEApy use pip\n",
    "#!pip install pangaeapy # Uncomment to upgrade pangaeapy\n",
    "\n",
    "## if you need to upgrade PANGAEApy use \n",
    "#!pip install pangaeapy --upgrade # Uncomment to upgrade pangaeapy\n",
    "\n",
    "## check version of PANGAEApy\n",
    "# !pip show pangaeapy\n",
    "\n",
    "## for details see https://pypi.org/project/pangaeapy/ \n",
    "\n",
    "import pangaeapy as pan\n",
    "from pangaeapy.pandataset import PanDataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PANGAEApy documentation\n",
    "To call the PANGAEApy documentation uncomment one of the following lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# help(pan)\n",
    "### or \n",
    "# help(pan.panquery)\n",
    "### or\n",
    "# help(pan.pandataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### ignore warnings in this script\n",
    "import warnings\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "warnings.simplefilter(action='ignore', category=(SettingWithCopyWarning))\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Query for data in PANGAEA\n",
    "\n",
    "AIM: How to search for datasets of a particular topic such as a species, location, project or author?\n",
    "\n",
    "This mirrors the query via the [PANGAEA website](https://pangaea.de/)  \n",
    "\n",
    "**Note:** The search term is enclosed with single quotes '. If your search term includes a blank, use additional double quotes \" inside the single quotes.  \n",
    "Example: _'sea ice'_ vs. _'\"sea ice\"'_  \n",
    "Example: _'parameter:Temperature, water method:CTD/Rosette'_ vs. _'parameter:\"Temperature, water\" method:CTD/Rosette'_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### General info on query\n",
    "* limit = the maximum number of datasets to be returned from query is 500.\n",
    "    * default limit = 10\n",
    "    * To download > 500 use the offset attribute e.g. pan.PanQuery(\"Triticum\", limit = 500, offset=500)\n",
    "* type: \n",
    "    * collection = dataset collection\n",
    "    * member = individual dataset which can be part of a dataset collection \n",
    "* score: Indicates how well the dataset matched the query search term\n",
    "* help(pan.panquery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Basic example queries\n",
    "* search via [keywords](https://wiki.pangaea.de/wiki/PANGAEA_search)\n",
    "* search via geographical coordinates a.k.a. bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query PANGAEA with 1 keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = pan.PanQuery('Geochemistry')\n",
    "### compare with https://pangaea.de/?q=Geochemistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### query is a PANGAEApy object with built in objects\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### you can ask the following attributes\n",
    "## totalcount, error, query, result\n",
    "print(query.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {query.totalcount} query results.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### put query results into dataframe\n",
    "query_results = pd.DataFrame(query.result)\n",
    "print(f'Total length of data frame query_results is {len(query_results)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query PANGAEA with combinations of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### find datasets that contain both \"Geochemistry\" and \"sediment core\"\n",
    "## remember how to use the different quotes:\n",
    "## The search term is enclosed with single quotes '. If your search term includes a blank, use additional double quotes \" inside the single quotes.\n",
    "query = pan.PanQuery('Geochemistry \"sediment core\"')\n",
    "print(f'There are {query.totalcount} query results.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional query terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### find datasets that contain \"Geochemistry\" and either \"Spitzbergen\" or \"Svalbard\" \n",
    "query = pan.PanQuery('Geochemistry AND (Spitzbergen OR Svalbard)')\n",
    "print(f'There are {query.totalcount} query results.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncertain spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### find datasets with uncertain spelling of single letter\n",
    "query = pan.PanQuery('Pal?nologic')\n",
    "print(f'There are {query.totalcount} query results.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specific author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### find datasets of author \"Boetius\"\n",
    "query = pan.PanQuery('citation:author:Boetius')\n",
    "print(f'There are {query.totalcount} query results.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Within geographical coordinates a.k.a bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### query database for \"Geochemistry\" and \"sediment core\" within a certain geolocation a.k.a. bounding box\n",
    "## bounding box: bbox=(minlon, minlat,  maxlon, maxlat)\n",
    "query = pan.PanQuery('Geochemistry \"sediment core\"', limit = 500, bbox=(-60, 50, -10, 70))\n",
    "print(f'There are {query.totalcount} query results.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 How to query PANGAEA without result limitations\n",
    "* The maximum of retrieving search results is 500 datasets.  \n",
    "* Retrieve datasets in chunks of 500 via offset option.  \n",
    "* Put all datasets in one data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get all results and combine them in data frame.\n",
    "\n",
    "### define search pattern\n",
    "search_pattern = 'project:label:PAGES_C-PEAT'\n",
    "\n",
    "### basic query to get number of search results\n",
    "query = pan.PanQuery(search_pattern, limit = 500)\n",
    "print(f'There are {query.totalcount} query results.')\n",
    "print(f'Currently query consists of {len(query.result)} entries.')\n",
    "\n",
    "### create empty data frame\n",
    "df_query_results_all = pd.DataFrame()\n",
    "\n",
    "### loop over all results in steps of 500\n",
    "for i in np.arange(0,query.totalcount,500):\n",
    "    \n",
    "    ### store result of individual step in qs\n",
    "    qs = pan.PanQuery(search_pattern, limit = 500, offset=i)\n",
    "    \n",
    "    ### convert qs result with 500 entries to data frame df_qs\n",
    "    df_qs = pd.DataFrame(qs.result)\n",
    "    \n",
    "    ### concatenate all individual df_qs into one data frame named query_results_all\n",
    "    df_query_results_all = pd.concat([df_query_results_all,df_qs],ignore_index=True)\n",
    "    \n",
    "print(f'df_query_results_all consists of {len(df_query_results_all)} results.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show first and last 3 lines\n",
    "pd.concat( [ df_query_results_all.head(3), df_query_results_all.tail(3) ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Quiz\n",
    "\n",
    "[More information](https://wiki.pangaea.de/wiki/PANGAEA_search) how to query with keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 How many datasets contain \"geological investigations\"?\n",
    "Hint: \"geological investigations\" **not** \"geological\" and \"investigations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### solution\n",
    "query = pan.PanQuery('\"geological investigations\"')\n",
    "print(query.totalcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 How many datasets contain \"geological investigations\" in the title only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### solution\n",
    "query = pan.PanQuery('citation:title:\"geological investigations\"')\n",
    "print(query.totalcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 How many datasets measured \"Temperature, water\" using a CTD/Rosette?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### solution\n",
    "query = pan.PanQuery('parameter:\"Temperature, water\" method:CTD/Rosette')\n",
    "print(query.totalcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get metadata of datasets\n",
    "\n",
    "A long list of metadata is callable with PanDataSet. \n",
    "Find a comprehensive list in internal documentation  \n",
    "_help(pan.PanQuery)_    \n",
    "or in this notebook full of examples: [pangaeapy_detailed_metadata_search.ipynb](https://github.com/pangaea-data-publisher/community-workshop-material/tree/master/Python/PANGAEApy_practical/pangaeapy_detailed_metadata_search.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Get metadata of individual dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example dataset from PANGAEA https://doi.pangaea.de/10.1594/PANGAEA.918423 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 ways to ask for dataset metadata\n",
    "\n",
    "## via URI\n",
    "# ds = PanDataSet('doi:10.1594/PANGAEA.918423', include_data=False)\n",
    "\n",
    "## via PANGAEA id number of dataset\n",
    "## id number can be either int or str\n",
    "# ds = PanDataSet('918423', include_data=False) \n",
    "ds = PanDataSet(918423, include_data=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic metadata retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Title\n",
    "ds.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Abstract\n",
    "ds.abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Authors\n",
    "print(f'Authors: {\"; \".join([x.fullname for x in ds.authors])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Full Reference\n",
    "ds.citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Geolocation\n",
    "print(f'Latitude: {ds.geometryextent[\"meanLatitude\"]}')\n",
    "print(f'Longitude: {ds.geometryextent[\"meanLongitude\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters\n",
    "params = \"; \".join([f'{param.name} [{param.unit}]' if param.unit else param.name for param in ds.params.values()])\n",
    "print(f'Parameters: {params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Event as dataframe\n",
    "ds.getEventsAsFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Event as PanEvent object\n",
    "print(ds.events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### information are stored as lists in PanEvent object\n",
    "print(type(ds.events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### therefore easy way of getting info is loop\n",
    "for event in ds.events:\n",
    "    print(event.label)\n",
    "    print(event.method.name)\n",
    "    print(event.basis.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store metadata in data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create empty data frame\n",
    "df = pd.DataFrame()\n",
    "\n",
    "### store metadata in df\n",
    "df.loc[0,'dataset title'] = ds.title\n",
    "df.loc[0,'abstract'] = ds.abstract\n",
    "\n",
    "### ds.authors is a list\n",
    "df.loc[0,'first author fullname'] = ds.authors[0].fullname\n",
    "df.loc[0,'all authors fullnames'] = \"; \".join([x.fullname for x in ds.authors])\n",
    "\n",
    "### authors orcids is a list\n",
    "df.loc[0,'all authors orcids'] = \"; \".join([x.ORCID if x.ORCID else \"no ORCID\" for x in ds.authors])\n",
    "\n",
    "df.loc[0,'citation'] = ds.citation\n",
    "df.loc[0,'dataset DOI'] = ds.doi\n",
    "df.loc[0,'west bound longitude'] = ds.geometryextent[\"westBoundLongitude\"]\n",
    "df.loc[0,'east bound longitude'] = ds.geometryextent[\"eastBoundLongitude\"]\n",
    "df.loc[0,'south bound latitude'] = ds.geometryextent[\"southBoundLatitude\"]\n",
    "df.loc[0,'north bound latitude'] = ds.geometryextent[\"northBoundLatitude\"]\n",
    "### parameters is a list\n",
    "df.loc[0,'parameters'] = \"; \".join([f'{param.name} [{param.unit}]' if param.unit else param.name for param in ds.params.values()])\n",
    "\n",
    "### event devices\n",
    "df.loc[0,'label'] = \"; \".join(set([device for device in ds.getEventsAsFrame()[\"label\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save dataframe as file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create data directory\n",
    "data_directory = \"PANGAEA_data\"\n",
    "# Check if it already exists before creating it\n",
    "if not os.path.isdir(data_directory):\n",
    "    os.mkdir(data_directory)\n",
    "    \n",
    "### Save as csv (comma seperated value)\n",
    "df.to_csv(os.path.join(data_directory, f'PANGAEA_metadata_{ds.id}.csv'), encoding='utf-8', index=False)\n",
    "df.to_csv(os.path.join(data_directory, f'PANGAEA_metadata_{ds.id}.txt'), sep='\\t', encoding='utf-8', index=False)\n",
    "print(f'PANGAEA metadata of \"{ds.title}\" saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Getting metadata for multiple datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define search pattern\n",
    "search_pattern = 'project:label:PAGES_C-PEAT and citation:title:geochemistry'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### do query, pay attention to limit\n",
    "query = pan.PanQuery(search_pattern, limit = 5)\n",
    "print(f'There are {query.totalcount} query results.')\n",
    "print(f'Currently query consists of {len(query.result)} entries.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### store query results in dataframe\n",
    "df = pd.DataFrame(query.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop over all entries in df and get metadata for each entry\n",
    "NOTE: As a safety precaution, the number of metadata requests is limited for a specific time period. \n",
    "\n",
    "_Received too many (metadata) requests error (429)...waiting 30s -_\n",
    "\n",
    "If you have larger requests, prepare to wait or use a different tool e.g. OAI-PMH (https://wiki.pangaea.de/wiki/OAI-PMH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create one data frame for all datasets\n",
    "data_all = pd.DataFrame()\n",
    "\n",
    "### loop over all datasets ins df\n",
    "for ind,value in df['URI'].items():\n",
    "    \n",
    "    ## use PanDataSet to get metadata and data and put them into 2 diferent dataframes\n",
    "    ds = PanDataSet(value, include_data=False)\n",
    "\n",
    "    print(ind, ds.doi)\n",
    "\n",
    "    ## put metadata into df in new columns\n",
    "    df.loc[ind,'Title'] = ds.title\n",
    "    df.loc[ind,'Publication date'] = ds.date\n",
    "    df.loc[ind,'Authors'] = {\"; \".join([x.fullname for x in ds.authors])}\n",
    "    df.loc[ind,'Citation'] = ds.citation\n",
    "    df.loc[ind,'DOI'] = ds.doi\n",
    "    df.loc[ind,'Parameters'] = \"; \".join([f'{param.name} [{param.unit}]' if param.unit else param.name for param in ds.params.values()])\n",
    "    if ds.events:\n",
    "        df.loc[ind,'Event'] = \"; \".join([x.label for x in ds.events])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### drop columns no longer needed\n",
    "df = df.drop(['URI','score','html','type','position'],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save dataframe as file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create data directory\n",
    "data_directory = \"PANGAEA_data\"\n",
    "### Check if it already exists before creating it\n",
    "if not os.path.isdir(data_directory):\n",
    "    os.mkdir(data_directory)\n",
    "    \n",
    "### Save as csv (comma seperated value)\n",
    "df.to_csv(os.path.join(data_directory, f'PANGAEA_metadata_df_all.csv'), encoding='utf-8', index=False)\n",
    "df.to_csv(os.path.join(data_directory, f'PANGAEA_metadata_df_all.txt'), sep='\\t', encoding='utf-8', index=False)\n",
    "print(f'PANGAEA metadata saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 What is the title of this dataset?\n",
    "https://doi.pangaea.de/10.1594/PANGAEA.804588"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### solution\n",
    "ds = PanDataSet(804588, include_data=False)\n",
    "ds.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 What is the name of the second author of this dataset?\n",
    "https://doi.pangaea.de/10.1594/PANGAEA.804588"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### solution\n",
    "ds = PanDataSet(804588, include_data=False)\n",
    "ds.authors[1].fullname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Did they measure pH in this dataset?\n",
    "https://doi.pangaea.de/10.1594/PANGAEA.743969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### solution\n",
    "ds = PanDataSet(\"743969\")\n",
    "list_params = list(ds.params)\n",
    "# print(list_params)\n",
    "if 'pH' in list_params:\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Download single dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example dataset: https://doi.pangaea.de/10.1594/PANGAEA.972802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = PanDataSet(972802)\n",
    "### ds contains data and metadata\n",
    "### see metadata section on how to get metadata\n",
    "type(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ds.data is data frame\n",
    "type(ds.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dataset header contains of parameter short names without unit\n",
    "ds.data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate to long parameter names\n",
    "Because by default parameters are abbreviated without units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Translate short parameters names to long names including unit\n",
    "def get_long_parameters(ds):\n",
    "    \"\"\"Translate short parameters names to long names including unit\n",
    "\n",
    "    Args:\n",
    "        ds (PANGAEA dataset): PANGAEA dataset\n",
    "    \"\"\"\n",
    "    ds.data.columns =  [f'{param.name} [{param.unit}]' if param.unit else param.name for param in ds.params.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_long_parameters(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create data directory\n",
    "data_directory = \"PANGAEA_data\"\n",
    "### Check if it already exists before creating it\n",
    "if not os.path.isdir(data_directory):\n",
    "    os.mkdir(data_directory)\n",
    "### Save as csv (comma seperated value)\n",
    "print(f'PANGAEA dataset \"{ds.title}\" saved')\n",
    "ds.data.to_csv(os.path.join(data_directory, f'PANGAEA_dataset_{ds.id}.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Download dataset including binary files e.g. images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example dataset: https://doi.pangaea.de/10.1594/PANGAEA.932826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download dataset from PANGAEA\n",
    "ds = PanDataSet(932826, enable_cache=True)\n",
    "### Spell out abbreviated parameters\n",
    "get_long_parameters(ds)\n",
    "ds.data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### download only 1 image\n",
    "df = ds.data[ds.data['DATE/TIME']=='2021-02-16 03:45:21']\n",
    "\n",
    "### Create file urls\n",
    "df[\"image_url\"] = [f'https://download.pangaea.de/dataset/{ds.id}/files/{img}' for img in df['Image']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create data directory\n",
    "data_directory = \"PANGAEA_data\"\n",
    "if not os.path.isdir(data_directory):\n",
    "    os.mkdir(data_directory)\n",
    "\n",
    "# ### download images\n",
    "# for i, file_url in enumerate(df['image_url']):\n",
    "#     response = requests.get(file_url,data_directory)    \n",
    "#     index = df.loc[(df == file_url).any(axis=1)].index[0]\n",
    "#     ### save image\n",
    "#     open(data_directory+'/'+df.loc[index,'Image'], 'wb').write(response.content)\n",
    "#     print(df.loc[index,'Image'] +' downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Download multiple datasets\n",
    "* download multiple datasets: data and metdata\n",
    "* combine data into one dataframe\n",
    "* combine metadata into one dataframe  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define search pattern and do query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### define search pattern\n",
    "search_pattern = 'project:label:PAGES_C-PEAT and citation:title:geochemistry' \n",
    "\n",
    "### do query\n",
    "query = pan.PanQuery(search_pattern, limit = 500)\n",
    "print(f'There are {query.totalcount} query results.')\n",
    "print(f'Currently query consists of {len(query.result)} entries.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all query results and combine them in data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### create empty data frame\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "### apply loop if query consists of more than 500 datasets\n",
    "if query.totalcount >= 500:\n",
    "    ### loop over all results in steps of 500\n",
    "    for i in np.arange(0,query.totalcount,500):\n",
    "        \n",
    "        ### store result of individual step in qs\n",
    "        qs = pan.PanQuery(search_pattern, limit = 500, offset=i)\n",
    "        \n",
    "        ### convert qs result with 500 entries to data frame df_qs\n",
    "        df_qs = pd.DataFrame(qs.result)\n",
    "        \n",
    "        ### concatenate all individual df_qs into one data frame named query_results_all\n",
    "        df_all = pd.concat([df_all,df_qs],ignore_index=True)\n",
    "else:\n",
    "    df_all = pd.DataFrame(query.result)\n",
    "\n",
    "print(f'There are {query.totalcount} query results.')\n",
    "print(f'df_all consists of {len(df_all)} results.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### show first 3 lines\n",
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[df_all['type']=='member']\n",
    "print(f'df_all consists of {len(df_all)} results.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical functions for complicated datasets\n",
    "* double parameter\n",
    "* method as comment  \n",
    "\n",
    "Example dataset: https://doi.pangaea.de/10.1594/PANGAEA.890478  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to find duplicate column names\n",
    "def find_duplicates(col_names):\n",
    "    name_counts = Counter(col_names)\n",
    "    duplicates = [name for name, count in name_counts.items() if count > 1]\n",
    "    return duplicates, bool(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to create general parameter long names with unit and method\n",
    "def generate_general_column_name(param):\n",
    "    base_name = f'{param.name} [{param.unit}]' if param.unit else param.name \n",
    "\n",
    "    if param.method:\n",
    "        base_name += f', method:{param.method.name}'\n",
    "\n",
    "    return base_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### functions to rename duplicate column names so they are all individual within the dataset\n",
    "def generate_unique_column_name(param):\n",
    "    base_name = f'{param.name} [{param.unit}]' if param.unit else param.name\n",
    "    \n",
    "    if param.method:\n",
    "        base_name += f', method:{param.method.name}'\n",
    "    \n",
    "    if param.comment:\n",
    "        return f'{base_name}, comment:{param.comment}'\n",
    "    else:\n",
    "        return f'{base_name}, col nr:{param.colno}'\n",
    "\n",
    "def make_unique_column_names(ds, same_param_name):\n",
    "    col_names = []\n",
    "    for param in ds.params.values():\n",
    "        name = generate_general_column_name(param)\n",
    "        if name in same_param_name:\n",
    "            name = generate_unique_column_name(param)\n",
    "        col_names.append(name)\n",
    "    return col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and combine data and metadata of query results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: As a safety precaution, the number of metadata requests is limited for a specific time period. \n",
    "\n",
    "_Received too many (metadata) requests error (429)...waiting 30s -_\n",
    "\n",
    "If you have larger requests, prepare to wait or use a different tool e.g. OAI-PMH (https://wiki.pangaea.de/wiki/OAI-PMH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create one data frame for all datasets\n",
    "data_all = pd.DataFrame()\n",
    "\n",
    "### loop over all datasets in df_all\n",
    "# for ind,value in df_all['URI'].items():\n",
    "### only download first 3 results during workshop\n",
    "for ind,value in df_all['URI'][0:3].items(): \n",
    "    \n",
    "    ## use PanDataSet to get metadata and data and put them into 2 diferent dataframes\n",
    "    ds = PanDataSet(value)\n",
    "\n",
    "    print(ind, ds.doi)\n",
    "\n",
    "    ## put metadata into df_all in new columns\n",
    "    df_all.loc[ind,'Title'] = ds.title\n",
    "    df_all.loc[ind,'Publication date'] = ds.date\n",
    "    df_all.loc[ind,'Authors'] = {\"; \".join([x.fullname for x in ds.authors])}\n",
    "    df_all.loc[ind,'Citation'] = ds.citation\n",
    "    df_all.loc[ind,'DOI'] = ds.doi\n",
    "    df_all.loc[ind,'Parameters'] = \"; \".join([f'{param.name} [{param.unit}]' if param.unit else param.name for param in ds.params.values()])\n",
    "    if ds.events:\n",
    "        df_all.loc[ind,'Event'] = \"; \".join([x.label for x in ds.events])\n",
    "    \n",
    "    ### Translate default short parameter names to long parameter names, add unit and method if available, check if all column names are individuals\n",
    "    col_names = []\n",
    "    for param in ds.params.values():\n",
    "        col_name = generate_general_column_name(param)\n",
    "        col_names.append(col_name)\n",
    "\n",
    "    ### find duplicate column names make them individual column names\n",
    "    same_param_name, double_name = find_duplicates(col_names)\n",
    "\n",
    "    if double_name:\n",
    "        col_names = make_unique_column_names(ds, set(same_param_name))\n",
    "    \n",
    "    ### rename columns because python cannot handle duplicate column names within dataframe\n",
    "    ds.data.columns =  col_names\n",
    "    \n",
    "    ### create new data dataframe for each query result \n",
    "    df_data = pd.DataFrame()\n",
    "    df_data = ds.data\n",
    "    df_data['DOI'] = ds.doi\n",
    "\n",
    "    ### combine all datasats into one dataframe\n",
    "    data_all = pd.concat([data_all,df_data], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### metadata table\n",
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rearrange and drop columns\n",
    "df_all = df_all[['Title','Event', 'Parameters', 'Citation', 'DOI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data table\n",
    "pd.concat([data_all.head(2),data_all.tail(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check header and merge columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show all header names\n",
    "data_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define parameter/header/column to be kept a.k.a. keep_param\n",
    "keep_param = 'Peat type, col nr:6'\n",
    "\n",
    "### copy value in keep column, if keep value is nan\n",
    "merge_param = 'Peat type, col nr:8'\n",
    "\n",
    "### if condition is needed because example consists of first 3 datasets\n",
    "if merge_param in data_all.columns and keep_param in data_all.columns:\n",
    "    ### merge merge_param into keep_param\n",
    "    mask = data_all[keep_param].isna() & data_all[merge_param].notna()\n",
    "    data_all.loc[mask, keep_param] = data_all.loc[mask, merge_param]\n",
    "    \n",
    "    ### remove merge_param\n",
    "    data_all = data_all.drop(columns=[merge_param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.rename(columns={'Peat type, col nr:6':'Peat type'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat( [data_all.head(2),data_all.tail(2)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_all[['DOI','Event', 'Latitude [deg]', 'Longitude [deg]', 'Elevation [m]',\n",
    "                     'DEPTH, sediment/rock [m]', 'AGE [ka BP]', 'Density, dry bulk [g/cm**3]',\n",
    "                     'Peat type, col nr:4', 'Peat type, col nr:8', 'Peat type, comment:Loisel et al. 2014',\n",
    "                     'Organic matter [%]','Density, organic matter [g/cm**3]','Density, organic carbon [g/cm**3]',\n",
    "                     'Carbon, total [%]','Nitrogen, total [%]']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([data_all.head(2),data_all.tail(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save dataframe as file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Create data directory\n",
    "data_directory = \"PANGAEA_data\"\n",
    "### Check if it already exists before creating it\n",
    "if not os.path.isdir(data_directory):\n",
    "    os.mkdir(data_directory)\n",
    "    \n",
    "### Save as tab delimited text file\n",
    "\n",
    "## set filename\n",
    "filename1 = 'PAGES_C-PEAT_Geochemistry_metadata.txt'\n",
    "filename2 = 'PAGES_C-PEAT_Geochemistry_data.txt'\n",
    "\n",
    "df_all.to_csv(os.path.join(data_directory, filename1), sep='\\t', encoding='utf-8', index=False)\n",
    "data_all.to_csv(os.path.join(data_directory, filename2), sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
